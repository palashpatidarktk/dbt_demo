{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20db8dac-d7ee-4466-b283-ae8cdf559c09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    BooleanType,\n",
    "    DateType,\n",
    "    FloatType,\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "inbound_folder = \"/Volumes/satya_test/dlt_test_incr_load/full_load/data/\"\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"userId\", IntegerType(), True),\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"body\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@dlt.view(comment=\"new users data incrementally ingested from landing zone\")\n",
    "def inbound_users():\n",
    "    df = (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .option(\"globPathFilter\", \"*.json\")\n",
    "        .schema(schema)\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .load(inbound_folder)\n",
    "    )\n",
    "    df = (\n",
    "        df.withColumn(\"_lh_ingestion_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"_lh_applicable_date\", F.current_date())\n",
    "        .withColumn(\"_lh_input_file_path\", F.col(\"_metadata.file_path\"))\n",
    "        .withColumn(\"_lh_file_mod_time\", F.col(\"_metadata.file_modification_time\"))\n",
    "        .withColumn(\"Operation\", F.lit(\"\"))\n",
    "    )  # added the operation Column\n",
    "    return df\n",
    "\n",
    "\n",
    "#data quality check    \n",
    "rules={\"valid_id\": \"userId IS NOT NULL\", \"valid_user\": \"id IS NOT NULL AND title IS NOT NULL\"}       \n",
    "quarantine_rules = \"NOT({0})\".format(\" AND \".join(rules.values()))\n",
    "\n",
    "\n",
    "@dlt.table(name=\"_raw_users_quantine\", comment=\"Raw data from landing zone\")\n",
    "@dlt.expect_all_or_drop(rules)\n",
    "def data_check():\n",
    "    return dlt.readStream(\"inbound_users\")\n",
    "\n",
    "\n",
    "dlt.create_streaming_table(\n",
    "    name=\"_raw_users\", \n",
    "    #Temporary=False,\n",
    "    table_properties={\n",
    "        \"delta.enableChangeDataFeed\": \"true\", #enabled CDF\n",
    "        \"delta.enableRowTracking\": \"true\", #enables row level tracking\n",
    "    },\n",
    ")\n",
    "pk = \"id\"\n",
    "dlt.expect_all_or_drop(rules)\n",
    "#implemented the _raw table as SCD-Type 2 table\n",
    "dlt.apply_changes(\n",
    "    target=\"_raw_users\",\n",
    "    source=\"inbound_users\",\n",
    "    keys=[pk],\n",
    "    stored_as_scd_type=\"2\",\n",
    "    sequence_by=\"_lh_file_mod_time\",\n",
    "    track_history_except_column_list=[\n",
    "        \"_lh_ingestion_timestamp\",\n",
    "        \"_lh_input_file_path\",\n",
    "        \"_lh_file_mod_time\",\n",
    "        \"_lh_applicable_date\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "@dlt.view\n",
    "def _raw_users_view():\n",
    "  return (spark.readStream\\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", True) \\\n",
    "    .table(\"live._raw_users\") \\\n",
    "    .filter((F.col(\"_change_type\")!=\"update_preimage\"))\n",
    "    .select(\"userId\",\"id\",\"title\",\"body\",\"__START_AT\",\"__END_AT\",F.col(\"_commit_timestamp\").alias(\"commit_timestamp\"))\n",
    "    .withColumnRenamed('__START_AT','Start_date')\n",
    "    .withColumnRenamed('__END_AT','End_date')\n",
    "    )\n",
    "\n",
    "\n",
    "dlt.create_streaming_table(name=\"users\",\n",
    "        comment= f\"Clean, merged data from the CDC\",\n",
    "        table_properties={\n",
    "        \"delta.enableChangeDataFeed\": \"true\", #enabled CDF\n",
    "        \"delta.enableRowTracking\": \"true\", #enables row level tracking\n",
    "    }\n",
    "        )\n",
    "\n",
    "dlt.apply_changes(\n",
    "  target = \"users\", #The customer table being materilized\n",
    "  source = \"_raw_users_view\", #the incoming CDC\n",
    "  keys = [pk], #Primary key to match the rows to upsert/delete\n",
    "  stored_as_scd_type = \"1\",\n",
    "  sequence_by = F.col(\"commit_timestamp\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Hydrate",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
