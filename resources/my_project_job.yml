# # The main job for my_project.
# resources:
#   jobs:
#     my_project_job:
#       name: my_project_job

#       schedule:
#         # Run every day at 8:37 AM
#         quartz_cron_expression: '44 37 8 * * ?'
#         timezone_id: Europe/Amsterdam

#       email_notifications:
#         on_failure:
#           - palash.patidar@koantekorg.onmicrosoft.com

#       tasks:
#         - task_key: notebook_task
#           job_cluster_key: job_cluster
#           notebook_task:
#             notebook_path: ../src/notebook.ipynb
        
#         - task_key: refresh_pipeline
#           depends_on:
#             - task_key: notebook_task
#           pipeline_task:
#             pipeline_id: ${resources.pipelines.my_project_pipeline.id}
        
#         - task_key: main_task
#           depends_on:
#             - task_key: refresh_pipeline
#           job_cluster_key: job_cluster
#           python_wheel_task:
#             package_name: my_project
#             entry_point: main
#           libraries:
#             # By default we just include the .whl file generated for the my_project package.
#             # See https://docs.databricks.com/dev-tools/bundles/library-dependencies.html
#             # for more information on how to add other libraries.
#             - whl: ../dist/*.whl

#       job_clusters:
#         - job_cluster_key: job_cluster
#           new_cluster:
#             spark_version: 13.3.x-scala2.12
#             node_type_id: Standard_D3_v2
#             autoscale:
#                 min_workers: 1
#                 max_workers: 4

# resources:
#   jobs:
#     dbt:
#       name: dbt
#       email_notifications:
#         on_failure:
#           - palash.patidar@koantek.com
#       tasks:
#         - task_key: dummy_run
#           notebook_task:
#             notebook_path: ../src/dummy.ipynb
#             source: WORKSPACE
#           min_retry_interval_millis: 900000
#           disable_auto_optimization: true
#       queue:
#         enabled: true




resources:
      

  jobs:

    Extract_data:
      name: Extract_data
      tasks:
        - task_key: itterator
          for_each_task:
            inputs: "[1,2,3]"
            concurrency: 3
            task:
              task_key: itterator_iteration
              notebook_task:
                notebook_path: ../src/Notebooks/ingest_data.ipynb
                base_parameters:
                  name: "{{input}}"
                source: WORKSPACE
      queue:
        enabled: true


    Test_run:
      name: Test_run
      tasks:
        - task_key: DataExtract_test
          run_job_task:
            job_id: ${resources.jobs.Extract_data.id}
            job_parameters:
              objects: "[1,2,3]"
        - task_key: Load
          depends_on:
            - task_key: DataExtract_test
          pipeline_task:
            pipeline_id: ${resources.pipelines.Hydrate.id}
      queue:
        enabled: true


